# conda activate py310 ollama run qwen3:1.7b pip3 install langchain_ollama
"""
# æ£€æŸ¥å½“å‰å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
ollama list

# å¦‚æœä½ æƒ³å°è¯• Qwen çš„æœ€æ–°å®éªŒç‰ˆï¼ˆå‡è®¾å« qwen3-previewï¼‰
ollama run qwen2.5:7b  # ç›®å‰æœ€æ¨è
# æˆ–è€…å°è¯•é˜¿é‡Œæœ€è¿‘å‘å¸ƒçš„æ›´å°ã€æ›´å¿«çš„ Coder ä¸“ç”¨ç‰ˆæœ¬
ollama run qwen2.5-coder:7b
"""

# æ ¸å¿ƒå˜åŒ–ï¼šå¯¼å…¥ Ollama é©±åŠ¨
import inspect
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage
from langchain.agents import create_agent
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.tools import BaseTool

# å¯¼å…¥å½“å‰æ–‡ä»¶å¤¹ä¸‹çš„ tools æ¨¡å—
import tools

def chat_loop():
    # 1. åˆå§‹åŒ– Ollama æ¨¡å‹
    # model="qwen2.5:7b" æ˜¯ç›®å‰æœ¬åœ°è¿è¡Œå·¥å…·è°ƒç”¨æœ€ç¨³çš„æ¨¡å‹ä¹‹ä¸€
    # å¦‚æœç”µè„‘é…ç½®ä¸€èˆ¬ï¼Œå¯ä»¥æ¢æˆ "qwen3:1.7b" 
    model = ChatOllama(
        model="qwen2.5:7b",
        # model="qwen3:1.7b",
        temperature=0, # è®¾ä¸º 0 ä»¥è·å¾—æ›´ç¡®å®šæ€§çš„å›ç­”
        streaming=True # å¼€å¯æµå¼è¾“å‡º
    )

    # 2. ç»„è£…å·¥å…·
    # langchain_tools = [tools.read_file, tools.list_files, tools.rename_file,tools.write_file]
    
    # è¿™ä¸€è¡Œä¼šè‡ªåŠ¨æŠ“å– tools.py é‡Œæ‰€æœ‰å¸¦ @tool çš„å‡½æ•°
    langchain_tools = [
        # ç»“æœå±‚ (Output)ï¼šobj æ•°æ®æºå±‚ (Source)ï¼šfor name, obj in inspect.getmembers(tools)  è¿‡æ»¤å™¨å±‚ (Filter)ï¼šif isinstance(obj, BaseTool)
        obj for name, obj in inspect.getmembers(tools) if isinstance(obj, BaseTool)
    ]

    # 3. è®¾ç½®è®°å¿†
    memory = MemorySaver()

    # 4. åˆ›å»º Agent
    system_prompt = "You are a professional Python programmer. Use the provided tools to manage files."
    agent_executor = create_agent(
        model=model, 
        tools=langchain_tools, 
        checkpointer=memory,
        system_prompt=system_prompt
    )

    # 5. è¿è¡Œå¾ªç¯ 
    config = {"configurable": {"thread_id": "local_user"}} 
    print("ğŸš€ Ollama Agent å·²å¯åŠ¨!")

    while True:
        user_input = input("\nUser >>> ")
        if user_input.lower() in ["exit", "quit"]:
            break

        print("AI >>> ", end="", flush=True)
        
        # è¿™é‡Œçš„ token å®é™…ä¸Šæ˜¯ä¸€ä¸ª AIMessageChunk
        for token, metadata in agent_executor.stream(
            {"messages": [HumanMessage(content=user_input)]},
            config=config,
            stream_mode="messages", # å…¶ä»–æ¨¡å¼è§æ–‡æ¡£
        ):
            # æ ¹æ®æ¨¡å‹ä¸åŒï¼Œå†…å®¹å¯èƒ½åœ¨ content æˆ– content_blocks é‡Œ
            # æ£€æŸ¥æ˜¯å¦æœ‰ content_blocks æ¨¡å‹å‘ç»™ä½ çš„æ¯ä¸€ä¸ª token ç¢ç‰‡å¹¶ä¸ä¸€å®šéƒ½å¸¦ç€content
            # token.content_blocks æ£€æµ‹content_blocksæ˜¯å¦ä¸ºç©ºã€‚ åœ¨ Python ä¸­ï¼Œç©ºåˆ—è¡¨ [] åœ¨å¸ƒå°”åˆ¤æ–­ä¸­è¢«è§†ä¸º False
            if hasattr(token, "content_blocks") and token.content_blocks:
                block = token.content_blocks[0]
                
                # 2. ç”¨å­—å…¸çš„æ–¹å¼è®¿é—® ['type'] å’Œ ['text']
                # å¢åŠ åˆ¤æ–­ï¼Œé˜²æ­¢æœ‰äº› block æ²¡æœ‰ text é”® 
                if isinstance(block, dict) and block.get("type") == "text":
                    print(block.get("text", ""), end="", flush=True)
            
            # 2. å¦‚æœä½ æƒ³çœ‹å½“å‰æ˜¯å“ªä¸ªèŠ‚ç‚¹åœ¨è¿è¡Œï¼ˆè°ƒè¯•ç”¨ï¼‰
            node_name = metadata.get('langgraph_node')
            if node_name == 'tools': # å¦‚æœæ­£åœ¨è¿è¡Œå·¥å…·èŠ‚ç‚¹
                print(f"\n[ğŸ› ï¸  æ‰§è¡Œå·¥å…·ä¸­...]", flush=True)

        print() # AI å›å¤ç»“æŸåæ¢è¡Œ

if __name__ == "__main__":
    chat_loop()