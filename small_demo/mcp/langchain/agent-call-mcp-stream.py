# pip install langchain langchain-openai langchain-google-genai
# conda activate py310 ; pip install --upgrade langchain langgraph
import asyncio
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.tools import Tool
from langchain_google_genai import ChatGoogleGenerativeAI # ç¤ºä¾‹ä½¿ç”¨ Gemini
from langchain_ollama import ChatOllama
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent
from langchain_core.messages import HumanMessage,AIMessage,AIMessageChunk


# 1. é…ç½® MCP æœåŠ¡å™¨å‚æ•° (ä¸ä¹‹å‰ä¸€è‡´)
server_params = StdioServerParameters(
    command="D:/application/anaconda3/envs/py310/python.exe",
    args=["D:/code/generativeAI/small_demo/mcp/langchain/ai-mcp-demo.py"],
    env={
        "PYTHONPATH": "D:/code/generativeAI/small_demo/mcp/langchain/",
        "PYTHONUNBUFFERED": "1"
    },
    cwd="D:/code/generativeAI/small_demo/mcp/langchain/",
    encoding="utf-8"
)

async def main():
    # ä½¿ç”¨ stdio_client è¿æ¥ MCP æœåŠ¡ç«¯
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            # æ¡æ‰‹åˆå§‹åŒ–
            await session.initialize()
            
            # è·å–å·¥å…·åˆ—è¡¨
            mcp_tools = await session.list_tools()
            
            # --- LangChain å·¥å…·è½¬æ¢å¼€å§‹ ---
            langchain_tools = []

            for tool in mcp_tools.tools:
                # ã€é—­åŒ…å¤„ç†ã€‘ä½¿ç”¨é»˜è®¤å‚æ•°é”å®š tool_name
                # LangChain çš„å·¥å…·å‡½æ•°é€šå¸¸æ¥æ”¶ä¸€ä¸ªå­—ç¬¦ä¸²æˆ–å­—å…¸ä½œä¸ºè¾“å…¥
                async def wrapper(tool_input=None, tool_name=tool.name):
                    """
                    LangChain å·¥å…·åŒ…è£…å™¨
                    tool_input: LangChain ä¼ å…¥çš„å‚æ•°ï¼ˆå¦‚æœå·¥å…·å£°æ˜ä¸éœ€è¦å‚æ•°ï¼Œåˆ™ä¸º Noneï¼‰
                    """
                    # å¦‚æœå·¥å…·ä¸éœ€è¦å‚æ•°ï¼Œarguments ä¼ ç©ºå­—å…¸ {}
                    arguments = tool_input if isinstance(tool_input, dict) else {}
                    
                    print(f"DEBUG: LangChain æ­£åœ¨è°ƒç”¨ MCP [{tool_name}] å·¥å…·ï¼Œå‚æ•°: {arguments}")
                    
                    # è½¬å‘è¯·æ±‚ç»™ MCP æœåŠ¡ç«¯ (C# æˆ–å…¶ä»– Python è„šæœ¬)
                    result = await session.call_tool(tool_name, arguments)
                    
                    # æå–æ–‡æœ¬ç»“æœè¿”å›ç»™ LangChain
                    if result.content:
                        return result.content[0].text
                    return "No output from tool."

                # å°†å…¶è½¬æ¢ä¸º LangChain çš„ Tool å¯¹è±¡ 
                # å¦‚æœä½ ç”¨çš„æ˜¯ ainvokeï¼ˆå¼‚æ­¥è°ƒç”¨ï¼‰ï¼Œå®ƒå°±å»æ‰¾ coroutine é‡Œçš„ wrapper å»å¹²æ´»ã€‚
                # å¦‚æœä½ ç”¨çš„æ˜¯ invokeï¼ˆåŒæ­¥è°ƒç”¨ï¼‰ï¼Œå®ƒå°±å»æ‰¾ funcã€‚
                lc_tool = Tool.from_function(
                    func=None,              # åŒæ­¥å‡½æ•°ç½®ç©º åŒæ­¥æ‰§è¡Œçš„å›é€€æ–¹æ¡ˆ å®ƒä¼šæ˜ç¡®å‘Šè¯‰ä½ â€œæˆ‘ä¸æ”¯æŒåŒæ­¥â€ï¼Œè€Œä¸æ˜¯æŠ¥ä¸€ä¸ªè«åå…¶å¦™çš„ç³»ç»Ÿé”™è¯¯ã€‚
                    coroutine=wrapper,      # ä¼ å…¥å¼‚æ­¥åŒ…è£…å™¨ å½“ä½ ï¼ˆAIï¼‰å†³å®šè°ƒç”¨æˆ‘æ—¶ï¼Œè¯·ä½¿ç”¨ await æ¥è¿è¡Œ wrapper è¿™ä¸ªå‡½æ•°
                    name=tool.name,         # å·¥å…·åç§°
                    description=tool.description # å·¥å…·æè¿°ï¼ŒAgent é è¿™ä¸ªåˆ¤æ–­ä½•æ—¶è°ƒç”¨
                )
                langchain_tools.append(lc_tool)
            # --- LangChain å·¥å…·è½¬æ¢ç»“æŸ ---

            # 2. åˆå§‹åŒ–æ¨¡å‹ (è¿™é‡Œä»¥ Gemini ä¸ºä¾‹)
            # llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash")
            llm = ChatOllama(
                model="qwen2.5:7b",
                temperature=0, # è®¾ä¸º 0 ä»¥è·å¾—æ›´ç¡®å®šæ€§çš„å›ç­”
                streaming=True # å¼€å¯æµå¼è¾“å‡º
            )

            # è®¾ç½®è®°å¿†
            memory = MemorySaver()
            
            # 4. æ„é€  Agent
            # placeholder ä¼šå‘Šè¯‰ LangChainï¼šâ€œè¿™é‡Œä¸æ˜¯å­˜ä¸€ä¸ªç®€å•çš„å­—ç¬¦ä¸²ï¼Œè€Œæ˜¯å­˜ä¸€ä¸ªæ¶ˆæ¯åˆ—è¡¨ã€‚â€ å½“ Agent è¿è¡Œæ—¶ï¼Œå®ƒä¼šå°†æ‰€æœ‰çš„å†å²æ¶ˆæ¯è‡ªåŠ¨å±•å¼€å¹¶å¡«å…¥è¿™ä¸ªä½ç½®ã€‚
            # ("placeholder", "{messages}")ï¼šæ¥æ”¶å¯¹è±¡åˆ—è¡¨ã€‚å®ƒä¿ç•™äº†æ¶ˆæ¯çš„åŸå§‹ç±»å‹ï¼ˆæ¯”å¦‚æŸæ¡æ¶ˆæ¯æ˜¯å·¥å…·è°ƒç”¨çš„æŒ‡ä»¤ï¼ŒæŸæ¡æ˜¯æ™®é€šå¯¹è¯ï¼‰ï¼Œè¿™å¯¹æ¨¡å‹åˆ¤æ–­åç»­åŠ¨ä½œè‡³å…³é‡è¦ã€‚
            # åœ¨æ„å»º Agent æ—¶ï¼Œplaceholder æ˜¯æ ‡å‡†é…ç½®ã€‚å¦‚æœå»æ‰å®ƒï¼Œä½ çš„ Agent æ¯æ¬¡åªèƒ½å¤„ç†å­¤ç«‹çš„ä¸€æ¡æŒ‡ä»¤ã€‚
            prompt = ChatPromptTemplate.from_messages([
                ("system", "You are a professional system manager assistant. You can use the provided tools to get host information as needed."),
                ("placeholder", "{messages}"),
            ])
            
            # create_agent è¿”å› AgentExecutorï¼Œastream è¿”å›å®Œæ•´ AIMessageã€‚
            # create_react_agent è¿”å› LangGraph çš„ CompiledGraphï¼Œastream_events è¿”å› AIMessageChunkï¼ˆç¢ç‰‡ï¼‰ã€‚
            # â˜…è®°ä½create_agentä¸æ”¯æŒastreamï¼Œè¿”å›çš„ä¸æ˜¯æµ
            agent_executor = create_react_agent(
                llm, 
                langchain_tools, 
                prompt=prompt,
                checkpointer=memory,
            )
            
            # G. é…ç½®è¿è¡Œå‚æ•°ï¼šthread_id ç”¨äºåŒºåˆ†ä¸åŒçš„ç”¨æˆ·æˆ–ä¼šè¯
            config = {"configurable": {"thread_id": "mcp_demo_session_001"}}
    
            print("\nDEBUG: Starting astream_events...")
            # 6. æ‰§è¡Œä»»åŠ¡ async  astream_events æ”¯æŒå¼‚æ­¥æµå¼è¾“å‡ºï¼Œè·å–æ›´ç»†ç²’åº¦çš„æµå¼äº‹ä»¶
            # ä½¿ç”¨ async for é…åˆ agent_executor.astream_events
            async for event in agent_executor.astream_events(
                {"messages": [HumanMessage(content="è¯·å¸®æˆ‘è·å–å½“å‰ä¸»æœºçš„ç³»ç»Ÿä¿¡æ¯")]},
                config=config,
                version="v2",# v2 æ˜¯å¯¹åº•å±‚å¼‚æ­¥æµé€»è¾‘çš„é‡å†™ï¼Œæ¯” v1 æ›´å¿«ï¼Œèµ„æºå ç”¨æ›´ä½ã€‚ ä¸ç”¨äº†è§£
            ):
                # è¿‡æ»¤äº‹ä»¶ç±»å‹ï¼Œåªå¤„ç† LLM æµå¼è¾“å‡º åŸºæœ¬ä¸ç”¨
                if event["event"] == "on_llm_stream":
                    # è·å– AIMessageChunk
                    chunk = event["data"]["chunk"]
                    if hasattr(chunk, "content") and chunk.content:
                        print(chunk.content, end="", flush=True)
                
                # å¤„ç†èŠå¤©æ¨¡å‹æµå¼ ï¼šç°ä»£ Agent å‡ ä¹éƒ½ç”¨è¿™ä¸ªï¼Œå› ä¸ºå®ƒèƒ½å¤„ç†å¤æ‚çš„å¯¹è¯é€»è¾‘ã€‚
                elif event["event"] == "on_chat_model_stream":
                    chunk = event["data"]["chunk"]
                    if hasattr(chunk, "content") and chunk.content:
                        print(chunk.content, end="", flush=True)
                
                # æ£€æµ‹å·¥å…·è°ƒç”¨
                elif event["event"] == "on_tool_start":
                    print(f"\n[ğŸ› ï¸  æ­£åœ¨å¼‚æ­¥è°ƒç”¨ MCP å·¥å…·: {event['name']}...]", flush=True)
                    # å¦‚æœä½ æƒ³çœ‹ AI ä¼ ç»™å·¥å…·çš„å‚æ•°ï¼š
                    # print(f"è¾“å…¥å‚æ•°: {event['data'].get('input')}")

                # ç›‘æ§å·¥å…·ç»“æŸ (çœ‹è¿”å›çš„ JSON)
                elif event["event"]  == "on_tool_end":
                    print(f"\n[âœ… å·¥å…·æ‰§è¡Œå®Œæ¯•: {event['name']}]")
                    
                    # æå–å·¥å…·è¿”å›çš„ç»“æœ
                    output = event["data"].get("output")
                    
                    # è¿™é‡Œçš„ output é€šå¸¸æ˜¯ ToolMessage
                    if hasattr(output, "content"):
                        print(f"è¿”å›ç»“æœ (JSON): \n{output.content}")
                    else:
                        print(f"è¿”å›ç»“æœ: {output}")

if __name__ == "__main__":
    # asyncioå®ƒèƒŒååšäº†ä¸‰ä»¶äº‹ï¼š
    #     åˆ›å»ºä¸€ä¸ªäº‹ä»¶å¾ªç¯ï¼ˆEvent Loopï¼Œå¯ä»¥ç†è§£ä¸ºå¼‚æ­¥ä»»åŠ¡è°ƒåº¦å™¨ï¼‰ã€‚
    #     è¿è¡Œä½ çš„ main() å‡½æ•°ã€‚
    #     è¿è¡Œç»“æŸåï¼Œè‡ªåŠ¨å…³é—­å¹¶æ¸…ç†è°ƒåº¦å™¨ã€‚
    asyncio.run(main()) 
    
    
"""
 åŸå…ˆå†™æ³•
  async for token, metadata in agent_executor.astream(
                {"messages": [HumanMessage(content="è¯·å¸®æˆ‘è·å–å½“å‰ä¸»æœºçš„ç³»ç»Ÿä¿¡æ¯ã€‚")]},
                config=config,
                stream_mode="messages", 
            ):
                if hasattr(token, "content_blocks") and token.content_blocks:
                    block = token.content_blocks[0]
                    if isinstance(block, dict) and block.get("type") == "text":
                        print(block.get("text", ""), end="", flush=True)
                        
                elif hasattr(token, "content") and token.content:
                    print(token.content, end="", flush=True)

                node_name = metadata.get('langgraph_node')
                if node_name == 'tools': 
                    print(f"\n[ğŸ› ï¸  æ­£åœ¨å¼‚æ­¥è°ƒç”¨ MCP å·¥å…·...]", flush=True)
            print("\n--- ä»»åŠ¡å®Œæˆ ---")
            

ç¬¬ä¸€æ¬¡è¾“å‡ºï¼ˆJSON éƒ¨åˆ†ï¼‰ï¼šè¿™æ˜¯ tools èŠ‚ç‚¹è¿è¡Œå®Œåï¼Œè¿”å›ç»™ AI çš„ ToolMessageï¼ˆå·¥å…·æ‰§è¡Œç»“æœï¼‰ã€‚
DEBUG: LangChain æ­£åœ¨è°ƒç”¨ MCP [get_host_info] å·¥å…·ï¼Œå‚æ•°: {}
{
    "system": "Windows",
    "release": "10",
    "machine": "AMD64",
    "processor": "AMD64",
    "memory_gb": "15.35",
    "cpu_count": "16",
    "cpu_model": "Unknown"
}



ç¬¬äºŒæ¬¡è¾“å‡ºï¼ˆæ–‡å­—éƒ¨åˆ†ï¼‰ï¼šè¿™æ˜¯ AI æ¥æ”¶åˆ°å·¥å…·ç»“æœåï¼Œç”Ÿæˆçš„ AIMessageï¼ˆæœ€ç»ˆå›å¤ï¼‰
[ğŸ› ï¸  æ­£åœ¨å¼‚æ­¥è°ƒç”¨ MCP å·¥å…·...]
å½“å‰ä¸»æœºçš„ç³»ç»Ÿä¿¡æ¯å¦‚ä¸‹ï¼š

- ç³»ç»Ÿ: Windows 10
- å†…å­˜: 15.35 GB
- CPU æ ¸å¿ƒæ•°: 16
- CPU æ¨¡å‹: æœªçŸ¥

å¦‚æœæœ‰å…¶ä»–éœ€è¦æŸ¥è¯¢çš„ä¿¡æ¯ï¼Œè¯·å‘ŠçŸ¥æˆ‘ã€‚
--- ä»»åŠ¡å®Œæˆ ---

å…³é”®åŒºåˆ«ï¼š

åŒæ­¥ streamï¼šè¿”å› AIMessageChunkï¼Œæ”¯æŒ token çº§æµå¼ï¼Œä½†è¦æ±‚å·¥å…·å¿…é¡»æ˜¯åŒæ­¥çš„ï¼ˆä¸æ”¯æŒå¼‚æ­¥å·¥å…·ï¼‰ã€‚
å¼‚æ­¥ astreamï¼šè¿”å›å®Œæ•´ AIMessageï¼Œä¸æ”¯æŒ token çº§æµå¼ï¼Œä½†æ”¯æŒå¼‚æ­¥å·¥å…·ã€‚
ä¸ºä»€ä¹ˆä½ çš„æƒ…å†µå¤±è´¥ï¼š
ä½ çš„å·¥å…·æ˜¯å¼‚æ­¥çš„ï¼ˆcoroutine=wrapperï¼‰ï¼ŒåŒæ­¥ stream å°è¯•åŒæ­¥è°ƒç”¨å·¥å…·ï¼Œå¯¼è‡´ NotImplementedError: Tool does not support sync invocation.ã€‚

è§£å†³æ–¹æ¡ˆï¼š

å¦‚æœè¦ç”¨åŒæ­¥ stream è·å– AIMessageChunkï¼Œéœ€è¦å°†å·¥å…·æ”¹ä¸ºåŒæ­¥ï¼ˆfunc=wrapper, coroutine=Noneï¼‰ï¼Œä½† MCP å®¢æˆ·ç«¯æ˜¯å¼‚æ­¥çš„ï¼Œæ‰€ä»¥æ— æ³•ç›´æ¥åŒæ­¥è°ƒç”¨ã€‚
æ¨èç»§ç»­ç”¨å¼‚æ­¥ astream_eventsï¼ˆLangGraphï¼‰ï¼Œå®ƒèƒ½æ­£ç¡®è¿”å› AIMessageChunk å¹¶æ”¯æŒå¼‚æ­¥å·¥å…·ã€‚
å¦‚æœä½ åšæŒç”¨ create_agent + åŒæ­¥æµå¼ï¼Œåªèƒ½ç”¨åŒæ­¥å·¥å…·ï¼Œä½† MCP ä¸æ”¯æŒåŒæ­¥è°ƒç”¨ã€‚å»ºè®®ç”¨ LangGraph çš„ create_react_agent + astream_eventsã€‚
"""