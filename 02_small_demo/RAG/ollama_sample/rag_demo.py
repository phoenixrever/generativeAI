"""
RAG æ¼”ç¤ºè„šæœ¬ 
"""

from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage, SystemMessage
import embed_ollamaEmbeddings as embed  # ç¡®ä¿ä½ çš„åµŒå…¥æ–‡ä»¶åæ­£ç¡®

# --- 1. é…ç½®ç”Ÿæˆæ¨¡å‹ ---
GENERATION_MODEL = "qwen2.5:7b"  # æˆ–è€…ä½ å®‰è£…çš„å…¶ä»–æ¨¡å‹

# åˆå§‹åŒ–èŠå¤©æ¨¡å‹ æ³¨æ„è¿™ä¸æ˜¯agent ä¸èƒ½ç”¨system prompt
llm = ChatOllama(
    model=GENERATION_MODEL,
    temperature=0,      # RAG é€šå¸¸è®¾ä¸º 0ï¼Œä¿è¯å›ç­”çš„ç¨³å®šæ€§
    streaming=True
)

def generate_answer(question: str, context_chunks: list[str]):
    """
    ç»“åˆä¸Šä¸‹æ–‡å’Œé—®é¢˜ï¼Œä½¿ç”¨ ChatOllama å®æ—¶æµå¼ç”Ÿæˆå›ç­”ã€‚
    """
    # 1. ç»„è£…ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
    context_text = "\n\n".join([f"èµ„æ–™ç‰‡æ®µ {i+1}:\n{chunk}" for i, chunk in enumerate(context_chunks)])
    print(f"[DEBUG]ğŸ“š æä¾›çš„ä¸Šä¸‹æ–‡:\n{context_text}\n{'-'*30}")
    
    # 2. å®šä¹‰ç³»ç»ŸæŒ‡ä»¤
    system_instruction = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åº“åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„ã€ä¸Šä¸‹æ–‡ã€å†…å®¹æ¥å›ç­”ã€é—®é¢˜ã€ã€‚\n"
        "è§„åˆ™ï¼š\n"
        "1. å¦‚æœä¸Šä¸‹æ–‡é‡Œæ²¡æœ‰ç­”æ¡ˆï¼Œè¯·ç›´æ¥è¯´â€œæ ¹æ®ç°æœ‰èµ„æ–™æ— æ³•å›ç­”â€ã€‚\n"
        "2. ä¸è¦ç¼–é€ äº‹å®ï¼Œä¿æŒå®¢è§‚ç®€æ´ã€‚\n"
        "3. å¦‚æœæ˜¯æ—¥æ–‡èµ„æ–™ï¼Œè¯·ç”¨ä¸­æ–‡æ€»ç»“æ ¸å¿ƒæ„å›¾ã€‚\n"
    )

    # 3. æ„å»ºæ¶ˆæ¯åˆ—è¡¨ (System + Human)
    messages = [
        SystemMessage(content=system_instruction),
        HumanMessage(content=f"--- ä¸Šä¸‹æ–‡ ---\n{context_text}\n\n--- é—®é¢˜ ---\n{question}\n\nå›ç­”ï¼š")
    ]

    # 4. æµå¼è°ƒç”¨
    try:
        print("âœ¨ å›ç­”ç»“æœ: ", end="", flush=True)
        full_response = ""
        
        # ä½¿ç”¨ .stream æ–¹æ³•
        for chunk in llm.stream(messages):
            content = chunk.content
            print(content, end="", flush=True) # å®æ—¶æ‰“å°å­—ç¬¦
            full_response += content
            
        print("\n" + "-" * 30)
        return full_response
    except Exception as e:
        raise Exception(f"ChatOllama æµå¼ç”Ÿæˆå¤±è´¥: {e}")

def main():
    question = "ä»¤ç‹å†²é¢†æ‚Ÿäº†ä»€ä¹ˆé­”æ³•ï¼Ÿ"
    print(f"ğŸ¤” é—®é¢˜: {question}")

    # æ£€ç´¢é˜¶æ®µ
    print("ğŸ” æ­£åœ¨ä»æ•°æ®åº“æ£€ç´¢ç›¸å…³èµ„æ–™...")
    chunks = embed.query_db(question, n_results=3)

    if not chunks:
        print("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³èµ„æ–™ã€‚")
        return

    # ç”Ÿæˆé˜¶æ®µ (æ­¤æ—¶å†…éƒ¨ä¼šè‡ªåŠ¨æµå¼æ‰“å°)
    try:
        generate_answer(question, chunks)
    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")

if __name__ == '__main__':
    main()