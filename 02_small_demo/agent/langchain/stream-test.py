# https://docs.langchain.com/oss/python/langchain/streaming#llm-tokens
from langchain.agents import create_agent
from langchain_ollama import ChatOllama

model = ChatOllama(
    # model="qwen2.5:7b",
    model="qwen3:1.7b",
    temperature=0,
    streaming=True
)
    
def get_weather(city: str) -> str:
    """Get weather for a given city."""

    return f"It's always sunny in {city}!"

agent = create_agent(
    model,
    tools=[get_weather],
)
# for chunk in agent.stream(  
#     {"messages": [{"role": "user", "content": "What is the weather in SF?"}]},
#     stream_mode="messages",
# ):
#     for step, data in chunk.items():
#         print(f"step: {step}")
#         print(f"content: {data['messages'][-1].content_blocks}")
        
        
# for token, metadata in agent.stream(  
#     {"messages": [{"role": "user", "content": "ä½ æ˜¯è°"}]},
#     stream_mode="messages",
# ):
#     # print(f"node: {metadata['langgraph_node']}")
#     print(f"content: {token.content_blocks[0].text}", end="", flush=True)
#     # print("\n")
    
    
for token, metadata in agent.stream(
    {"messages": [{"role": "user", "content": "ä½ æ˜¯è°"}]},
    stream_mode="messages",
):
    # 1. æ£€æŸ¥æ˜¯å¦æœ‰ content_blocks
    if hasattr(token, "content_blocks") and token.content_blocks:
        block = token.content_blocks[0]
        
        # 2. ç”¨å­—å…¸çš„æ–¹å¼è®¿é—® ['type'] å’Œ ['text']
        # å¢åŠ åˆ¤æ–­ï¼Œé˜²æ­¢æœ‰äº› block æ²¡æœ‰ text é”®
        if isinstance(block, dict) and block.get("type") == "text":
            print(block.get("text", ""), end="", flush=True)
            
    # 3. (å¯é€‰) å¦‚æœä½ æƒ³å¤„ç†å·¥å…·è°ƒç”¨ï¼Œå®ƒä»¬é€šå¸¸ä¸åœ¨ content_blocks é‡Œ
    # if hasattr(token, "tool_call_chunks") and token.tool_call_chunks:
    #     print("\nğŸ› ï¸ [æ­£åœ¨æ„é€ å·¥å…·è°ƒç”¨...]", end="", flush=True)