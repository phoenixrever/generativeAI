这是一个非常敏锐的发现！你对比出了 LangChain 中 **“新旧 API”** 以及 **“不同流式模式 (Stream Mode)”** 的核心差异。

简单来说，你看到的这两个例子就像是同一个火车站的“普通进站口”和“VIP 快速通道”，它们的**数据结构（Data Structure）和返回节奏**完全不同。

---

### 1. 核心差异：`messages` vs `updates`

你在代码中使用的 `stream_mode` 决定了 AI 给你的“包裹”里装的是什么：

| **模式**       | **返回的内容 (chunk 的结构)**                                | **特点**                                                                       |
| -------------- | ------------------------------------------------------------ | ------------------------------------------------------------------------------ |
| **`messages`** | 每一块都是一个**Message 对象** （或其碎片）。                | **最细粒度** 。AI 每说一个字或决定用工具，都会立即推给你。适合做“打字机”效果。 |
| **`updates`**  | 每一块是一个**Dict (字典)** ，包含该步骤产生的完整消息列表。 | **按步结算** 。它会等 AI 把一整句话说完，或者等工具执行完，才给你一个总结包。  |

---

### 2. 为什么你的代码结构不一样？

让我们拆解一下文档里的那个例子：

**Python**

```
for chunk in agent.stream(..., stream_mode="updates"):
    for step, data in chunk.items():
        # data["messages"][-1] 拿到的是这一步最后产生的完整消息
```

- **文档例子（Updates）** ：它是按“节点”返回的。比如：第一步（AI 思考），第二步（执行工具），第三步（AI 总结）。每一步都是一个完整的 JSON 包。
- **你的代码（Messages）** ：它是按“碎片”返回的。每当你收到一个 `msg`，它可能只是 AI 回复中的一个单词。所以你需要用 `print(end="", flush=True)` 把碎片拼起来。

---

### 3. 关键术语：`content` vs `content_blocks`

文档例子中出现的 `content_blocks` 是 **LangChain 最新的长文本/多模态消息格式** ：

- **`msg.content`** ：通常是一个简单的字符串（String）。对于大多数 Ollama 模型，这是最直接的获取方式。
- **`msg.content_blocks`** ：是一个列表（List），里面可能包含文本块、图片块、甚至思维链（Reasoning）块。

> **注意：** 并不是所有模型都支持 `content_blocks`。你的本地 `qwen2.5` 主要是返回标准的 `content` 字符串。

---

### 4. 总结：我该用哪种？

如果你希望用户体验好， **坚持用你现在的 `messages` 模式** 。

- **`messages` 模式（你的代码）** ：
- ✅ 响应极快（用户立刻看到第一个字）。
- ✅ 适合本地 3060 显卡（显存有限时，流式输出能掩盖推理的延迟）。
- **`updates` 模式（文档代码）** ：
- ✅ 逻辑清晰（容易看到 Agent 运行到了哪一步）。
- ❌ 体验稍差（用户需要等 AI 把一段话全部想完才能看到结果）。

---

### 5. 给你的修正建议

如果你想让你的代码兼容文档那种“结构化”的感觉，但保留流式体验，你可以这样修改你的 `astream` 循环：

**Python**

```
# 如果你想看“步骤”，可以把模式切回 updates，但要忍受文字不再一个一个蹦出来
async for chunk in agent_executor.astream(inputs, config=config, stream_mode="updates"):
    for node_name, data in chunk.items():
        print(f"\n[节点: {node_name}]") # 比如这里会显示 'agent' 或 'tools'
        if "messages" in data:
            last_msg = data["messages"][-1]
            print(f"内容: {last_msg.content}")
```

**不过，对于 Ollama 这种本地模型，我强烈建议保留你目前的 `stream_mode="messages"` 写法。它最符合人类的聊天习惯。**

这是一个非常好的问题。理解了这两个点，你就理解了 LLM 流式输出中“ **脏数据处理** ”的精髓。

### 1. 为什么 `content_blocks` 会是空数组？

在流式输出中，模型发给你的每一个 `token` 碎片并不一定都带着“话”。

- **元数据块** ：有时候模型会先发一个“握手”信号，或者是一个包含 ID、运行节点信息的空块，告诉你“我要开始说话了”或者“我这部分说完了”，但里面没有实际内容。
- **工具调用块** ：如果模型决定调用工具（比如改文件名），它产生的碎片里可能只有 `tool_call_chunks`，而没有 `content_blocks`（文字块）。

如果不加 if token.content_blocks: 判断：

当程序遇到一个空的碎片，你尝试访问 token.content_blocks[0]，Python 就会直接报 IndexError: list index out of range（下标越界错误），你的程序就崩了。

---

### 2. 为什么判断 `isinstance(block, dict)`？

这是为了 **防御式编程** ，应对不同版本的“变脸”。

LangChain 和 Ollama 之间的传输协议有时会变化：

- **模式 A（字典）** ：`block` 是一个普通的 Python 字典，访问方式是 `block.get("type")`。
- **模式 B（对象）** ：`block` 是一个类实例，访问方式应该是 `block.type`。

如果某一天 LangChain 升级了，把字典变成了对象，而你直接用 `block.get("type")`，程序会报 **`AttributeError`** 。通过 `isinstance` 判断，你可以确保代码在两种情况下都能跑（或者至少在当前字典模式下不报错）。

---

### 3. 为什么判断 `block.get("type") == "text"`？

这是为了 **过滤非文字内容** 。

在一个 `content_blocks` 列表里，可能包含不同类型的块。虽然目前你只看到了文字，但未来可能会有：

- `{"type": "text", "text": "你好"}`
- `{"type": "image_url", "image_url": "..."}`（多模态模型）
- `{"type": "thought", "thought": "我该不该删文件呢？"}`（深度思考模型如 DeepSeek-R1）

如果你不判断 `type == "text"` 而是直接打印，你可能会把图片的 URL 或者模型内部的“内心戏”也打印到屏幕上，导致界面乱码。

---

### 4. 优化后的“原生异步”方案（不带 aioconsole）

很多朋友不想为了一个 `input` 去安装 `aioconsole`。我们可以使用 `asyncio.to_thread` 在独立线程运行同步 `input`，这样就不会锁死主线程的异步流了。

**Python**

```
import asyncio
import sys
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
import tools

# 原生异步输入包装
async def ainput(prompt: str) -> str:
    return await asyncio.to_thread(input, prompt)

async def chat_loop():
    model = ChatOllama(model="qwen3:1.7b", streaming=True, temperature=0)
    agent_executor = create_react_agent(
        model,
        tools=[tools.read_file, tools.list_files, tools.rename_file],
        checkpointer=MemorySaver()
    )
    config = {"configurable": {"thread_id": "async_v1"}}

    while True:
        # 使用自定义的异步 ainput
        user_input = await ainput("\nUser >>> ")
        if user_input.lower() in ["exit", "quit"]: break

        print("AI >>> ", end="", flush=True)

        async for token, metadata in agent_executor.astream(
            {"messages": [HumanMessage(content=user_input)]},
            config=config,
            stream_mode="messages"
        ):
            # 更加优雅的防御性写法
            blocks = getattr(token, "content_blocks", [])
            if blocks:
                block = blocks[0]
                # 兼容字典获取 text
                if isinstance(block, dict) and block.get("type") == "text":
                    print(block.get("text", ""), end="", flush=True)
                # 兼容对象获取 text
                elif hasattr(block, "text"):
                    print(block.text, end="", flush=True)

        print()

if __name__ == "__main__":
    asyncio.run(chat_loop())
```

**简单总结：**

- `if token.content_blocks:` 是为了 **不崩** 。
- `isinstance` 是为了 **兼容** 。
- `type == "text"` 是为了 **不乱** 。

**你现在跑这个异步版本，是不是感觉逻辑更清晰了？如果你问它“帮我看看当前目录有什么文件”，你应该能看到它先蹦出文字，然后执行工具，最后再反馈结果。**
